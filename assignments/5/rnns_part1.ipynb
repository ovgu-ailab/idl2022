{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-Chp8YyyvxN"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVE7aMvC4iXK"
   },
   "outputs": [],
   "source": [
    "# remove infrequent words. you can play with this parameter as it will likely impact model quality\n",
    "num_words = 20000\n",
    "(train_sequences, train_labels), (test_sequences, test_labels) = tf.keras.datasets.imdb.load_data(num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c07u7Z7s4opk"
   },
   "outputs": [],
   "source": [
    "# look at some sequences. words have been replaced with arbitrary index mappings\n",
    "# 1 is a special \"beginning of sequence\" marker\n",
    "# infrequent words have been replaced by the index 2\n",
    "# actual words start with index 4, 3 is never used (???)\n",
    "train_sequences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AD6Elit34sTL"
   },
   "outputs": [],
   "source": [
    "# labels are simply binary: sentiment can be positive or negative\n",
    "train_labels[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHTMEyXW5KcQ"
   },
   "outputs": [],
   "source": [
    "# to restore words, load the word-to-index mapping\n",
    "word_to_index = tf.keras.datasets.imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vv25lUc_5ckG"
   },
   "outputs": [],
   "source": [
    "# invert to get index-to-word mapping\n",
    "index_to_word = dict((index, word) for (word, index) in word_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYX6F3AX5hpV"
   },
   "outputs": [],
   "source": [
    "# we can convert a sequence to text by\n",
    "# - replacing each index by the respective word\n",
    "# - joining words together via spaces\n",
    "# note that we remove the beginning of sequence character and we have to subtract 3 from all indices\n",
    "# this is because, as mentioned above, the smallest indices are reserved for special characters\n",
    "# but for some reason this is not reflected in the mapping...\n",
    "\" \".join([index_to_word.get(index - 3, \"UNKNOWN\") for index in train_sequences[0][1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9axnbnwR6q6W"
   },
   "outputs": [],
   "source": [
    "# we cannot create a dataset :( this is because sequences are different length\n",
    "# but tensors have to be \"rectangular\"\n",
    "train_data = tf.data.Dataset.from_tensor_slices(train_sequences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2lt9mE-9XO7"
   },
   "outputs": [],
   "source": [
    "# solution is padding all sequences to the maximum length.\n",
    "# first find the maximum length\n",
    "sequence_lengths = [len(sequence) for sequence in train_sequences]\n",
    "max_len = max(sequence_lengths)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "677ZXcRu9nUe"
   },
   "outputs": [],
   "source": [
    "# overview over sequence lengths in the data\n",
    "# could also look at mean, median, standard deviation...\n",
    "plt.hist(sequence_lengths, bins=80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYr10G5M9rWX"
   },
   "outputs": [],
   "source": [
    "# luckily there is a convenient function for padding\n",
    "train_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXEICggj-OL-"
   },
   "outputs": [],
   "source": [
    "# now we can create a dataset!\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_sequences_padded, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPTPy5Ff-Q_C"
   },
   "outputs": [],
   "source": [
    "# all sequences are... very long\n",
    "train_sequences_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ug0OSIGjf6ji"
   },
   "outputs": [],
   "source": [
    "# it would be better to do something like this\n",
    "# all sequences above maxlen will be truncated to that length\n",
    "# note: pad_sequences has \"pre\" and \"post\" options for both padding and truncation. one may be better than the other!\n",
    "train_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=200)\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_sequences_padded, train_labels))\n",
    "\n",
    "train_sequences_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZW7YdDv_fRJ"
   },
   "outputs": [],
   "source": [
    "# for fun, you can look at the word-index mappings.\n",
    "# in this case, the mapping was done according to word frequency.\n",
    "# you can pass reverse=True to sorted() to look at the least common words.\n",
    "sorted(index_to_word.items())[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4fwUhqBACri"
   },
   "outputs": [],
   "source": [
    "# here is a high-level sketch for training RNNs\n",
    "\n",
    "\n",
    "# training loop -- same thing as before!!\n",
    "# our data is now slightly different (each batch of sequences has a time axis, which is kinda new)\n",
    "# but all the related changes are hidden away at lower levels\n",
    "def train_loop():\n",
    "    for sequence_batch, label_batch in train_data:\n",
    "        train_step(sequence_batch, label_batch)\n",
    "\n",
    "\n",
    "# a single training step -- again, seems familiar?\n",
    "def train_step(sequences, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = rnn_loop(sequences)\n",
    "        loss = loss_fn(labels, logits)\n",
    "\n",
    "    gradient = ...\n",
    "    apply_gradients(...)\n",
    "\n",
    "\n",
    "# here's where things start to change\n",
    "# we loop over the input time axis, and at each time step compute the new\n",
    "# hidden state based on the previous one as well as the current input\n",
    "# the state computation is hidden away in the rnn_step function and could be\n",
    "# arbitrarily complex.\n",
    "# in the general RNN, an output is computed at each time step, and the whole\n",
    "# sequence is returned. but in this case, since we only have one label for the\n",
    "# entire sequence, we only use the final state to compute one output and return it.\n",
    "# before the loop, the state need to be initialized somehow.\n",
    "def rnn_loop(sequences):\n",
    "    old_state = ...\n",
    "\n",
    "    for step in range(max_len):\n",
    "        x_t = sequences[:, step]\n",
    "        x_t = tf.one_hot(x_t, depth=num_words)\n",
    "        new_state = rnn_step(old_state, x_t)\n",
    "\n",
    "        old_state = new_state\n",
    "\n",
    "    o_t = output_layer(new_state)\n",
    "\n",
    "    return o_t\n",
    "\n",
    "\n",
    "# see formulas in the book ;)\n",
    "def rnn_step(state, x_t):\n",
    "    ..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMNRrNJS3Dbr71CnFVK3jax",
   "collapsed_sections": [],
   "name": "rnns_part1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
