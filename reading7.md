---
layout: default
title: Reading Assignment
id: reading7
---


# Reading Assignment: Word Embeddings & Language Models



Besides the image processing domain that we already covered extensively (e.g. in [reading 3](reading3.html)), deep learning has also had a major impact on the field of Natural Language Processing (NLP).
The following references provide a good overview.
Feel free to dive deeper if you are interested!

* [Sebastian Ruder's blog post "A Review of the Neural History of Natural Language Processing"](https://ruder.io/a-review-of-the-recent-history-of-nlp/) gives a nice introductory overview of the most important deep learning developments in NLP that nicely connects with the topics covered in the previous lectures.

* Next, read the two overview blog posts by Lilian Weng ["Learning Word Embedding"](https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html) and ["Generalized Language Models"](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html).

## Further Reading (Optional)

* Sebastian Ruder describes the implications of models like BERT for NLP in his [block post "NLP's ImageNet moment has arrived"](https://ruder.io/nlp-imagenet/).
* For further in-depth reading, follow the links from the blog posts to the referenced articles.

